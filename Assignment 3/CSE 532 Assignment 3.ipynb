{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "This assignment deals with using `textblob` and other open-source libraries to perform NLP-based analysis on documents using Python.  **All parts should use the same three documents (as outlined in Part 1 below).  In addition to your .ipynb and/or .py files, you must submit the three documents in .txt format, as well as a report document in .txt/.pb format that answers various questions below.  It is not necessary to submit the .csv file for Part 3, since we will be executing your code.  _Just make sure it works correctly!_**  \n",
    "\n",
    "**Part 1:**<br> Select and download three texts of your choosing that represent different media or writing formats (for example, you could choose i. a novel, movie script, and play script or ii. a short story, poem, and novel, etc.)\n",
    "**Make sure you briefly descibe your documents and explain the difference between them in a paragraph.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part 2:**<br>\n",
    "(a) Compute word counts for each of your documents after excluding English stop words (and optionally, performing lemmatization and/or other preprocessing that you would like to employ).<br>\n",
    "(b) Create and display a bar plot for each document that include word counts for the 25 most frequent words (after the above processing).<br>\n",
    "(c) Create and display a word cloud for each document (using a mask image of your choice) that includes only the 100 most frequent words.  Note that you'll likely want to use the approach outlined in Session 25 that utilizes the `fitwords` method, since you will want data consistent with those for part (b).<br>(d) Do you see any notable difference between the documents wrt (b) and/or (c) above?  Try to explain why or why not, and whether or not these results are expected.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import TextIOWrapper\n",
    "import itertools\n",
    "import string\n",
    "from typing import OrderedDict\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "prince = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "interstellar = open('interstellar.txt', 'r', encoding='utf-8-sig')\n",
    "romeojuliet = open('pg1513.txt', 'r', encoding='utf-8-sig')\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "def read_file(file: TextIOWrapper): # read file and return list of words\n",
    "    words = []\n",
    "    for line in file:\n",
    "        for word in line.split():\n",
    "            words.append(word.lower())\n",
    "    return words\n",
    "\n",
    "def preprocess(text):\n",
    "    for i in range(len(text)):\n",
    "        text[i] = text[i].strip(string.punctuation) # remove punctuation\n",
    "\n",
    "    stopwords = open('stop_words.txt', 'r')\n",
    "    exec(stopwords.read(), globals())\n",
    "    stopwords.close()\n",
    "    remove_blanks = [word for word in text if word != ''] # remove blanks\n",
    "    usablewords = [word for word in remove_blanks if word not in stop_words] # remove stopwords\n",
    "    return usablewords\n",
    "\n",
    "def word_count(text): # count number of words in list (Part A)\n",
    "    print(len(text))\n",
    "\n",
    "def bar_plot(text): # create bar plot of word frequency (Part B)\n",
    "    dataframe = pd.DataFrame(text, columns=['word'])\n",
    "    fig, ax = plt.subplots() \n",
    "    counts = dataframe['word'].value_counts().head(25) # get top 25 words\n",
    "    counts.plot(kind='bar', ax=ax) # plot horizontal bar chart\n",
    "    ax.set_ylabel('Word') \n",
    "    ax.set_xlabel('Word count')\n",
    "    plt.show() # show plot in order\n",
    "    \n",
    "def wordcloud(text): # Create word cloud of 100 most common words (Part C)\n",
    "    dataframe = pd.DataFrame(text, columns=['word'])\n",
    "    sorted_items = dataframe['word'].value_counts().to_dict() # sort by frequency\n",
    "    sorted_items = OrderedDict(itertools.islice(sorted_items.items(), 100))\n",
    "    mask_image = imageio.v2.imread('cloud.png') # cloud mask image \n",
    "    wordcloud = WordCloud(width=1000, height=1000, mask=mask_image, background_color='white') # create word cloud\n",
    "    wc = wordcloud.fit_words(sorted_items) # fit words to cloud\n",
    "    plt.imshow(wc) \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    prince_words = read_file(prince)\n",
    "    prince_processed = preprocess(prince_words)\n",
    "    print('The word count of The Prince is:')\n",
    "    word_count(prince_processed)\n",
    "    bar_plot(prince_processed)\n",
    "    wordcloud(prince_processed)\n",
    "    interstellar_words = read_file(interstellar)\n",
    "    interstellar_processed = preprocess(interstellar_words)\n",
    "    print('The word count of Interstellar is:')\n",
    "    word_count(interstellar_processed)\n",
    "    bar_plot(interstellar_processed)\n",
    "    wordcloud(interstellar_processed)\n",
    "    romeojuliet_words = read_file(romeojuliet)\n",
    "    romeojuliet_processed = preprocess(romeojuliet_words)\n",
    "    print('The word count of Romeo and Juliet is:')\n",
    "    word_count(romeojuliet_processed)\n",
    "    bar_plot(romeojuliet_processed)\n",
    "    wordcloud(romeojuliet_processed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part 3:**<br>\n",
    "(a) Using your approach from **Part 2**, compute the 25 most _cumulatively commmon_ words across the three documents, along with the _cumulative counts_.  Remember that a given word can appear in 2 or even all 3 documents.  <br>\n",
    "Ex: if the word \"spider\" appears 10 times in document 1, 6 times in document 2, and 5 times in document 3, its cumulative count will be 21.<br>\n",
    "(b) Create a CSV file named **MCW.csv** with the following specifications:\n",
    "i. The csv file should use the standard delimiter (,) <br> \n",
    "ii. The first row in the file should be a column header row denoted by the string \"Word,Count\" <br>\n",
    "iii. The next 25 rows should be populated with the pairs of the 25 most cumulatively common words and counts, in descending order by count. <br>\n",
    "iv. One final row should added of the form \"Sum,(totalcount)\" where (totalcount) represents the sum of the top 25 cumulative counts.<br>\n",
    "A sample csv file is included to give you an idea of what to generate in practice.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prince = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "interstellar = open('interstellar.txt', 'r', encoding='utf-8-sig')\n",
    "romeojuliet = open('pg1513.txt', 'r', encoding='utf-8-sig')\n",
    "\n",
    "def cummulative_counts(text): # get top 25 words in all texts (Part A)\n",
    "    dataframe = pd.DataFrame(text, columns=['word'])\n",
    "    counts = dataframe['word'].value_counts().head(25) # get top 25 words\n",
    "    print(counts)\n",
    "\n",
    "def most_common(text):\n",
    "    dataframe = pd.DataFrame(text, columns=['word'])\n",
    "    counts = dataframe['word'].value_counts().head(25) # get top 25 words\n",
    "    counts.to_csv('MCW.csv')\n",
    "    csv = open('MCW.csv', 'a')\n",
    "    csv.write('Sum,' + str(counts.sum()))\n",
    "\n",
    "def main():\n",
    "    prince_words = read_file(prince)\n",
    "    prince_processed = preprocess(prince_words)\n",
    "    interstellar_words = read_file(interstellar)\n",
    "    interstellar_processed = preprocess(interstellar_words)\n",
    "    romeojuliet_words = read_file(romeojuliet)\n",
    "    romeojuliet_processed = preprocess(romeojuliet_words)\n",
    "    all_words = prince_processed + interstellar_processed + romeojuliet_processed\n",
    "    bar_plot(all_words)\n",
    "    cummulative_counts(all_words)\n",
    "    most_common(all_words)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4:**<br>\n",
    "(a) Use **Textatistic** to compute the _average_ of the Flesch–Kincaid, Gunning Fog, SMOG, and Dale–Chall scores for each document.   \n",
    "(b) Are there noticeable differences among your documents's readability scores, and would you expect these differences (or lack of differences, if there are none) to be present among documents were you judging their readability manually?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textatistic import Textatistic\n",
    "from pathlib import Path\n",
    "\n",
    "prince = Path('pg1232.txt').read_text(encoding='utf-8-sig')\n",
    "interstellar = Path('interstellar.txt').read_text(encoding='utf-8-sig')\n",
    "romeojuliet = Path('pg1513.txt').read_text(encoding='utf-8-sig')\n",
    "\n",
    "def average_scores(text):\n",
    "    readability = Textatistic(text)\n",
    "    average_score = (readability.fleschkincaid_score + readability.gunningfog_score + readability.smog_score + readability.dalechall_score) / 4\n",
    "    print('The average readability score is:', average_score)\n",
    "    return average_score\n",
    "\n",
    "prince_score = average_scores(prince)\n",
    "interstellar_score = average_scores(interstellar)\n",
    "romeojuliet_score = average_scores(romeojuliet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part 5:**<br> \n",
    "(a) Use spaCy to compute the pairwise similarity between your documents (i.e. doc. 1 to doc. 2, doc. 1 to doc. 3, doc. 2 to doc. 3).<br>\n",
    "(b) Do any of these similarity scores seem higher or lower than you would expect?  Explain your response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp_sm = spacy.load('en_core_web_lg')\n",
    "\n",
    "prince = nlp_sm(Path('pg1232.txt').read_text(encoding='utf-8-sig')) # doc 1\n",
    "interstellar = nlp_sm(Path('interstellar.txt').read_text(encoding='utf-8-sig')) # doc 2\n",
    "romeojuliet = nlp_sm(Path('pg1513.txt').read_text(encoding='utf-8-sig')) # doc 3\n",
    "\n",
    "docsim12=prince.similarity(interstellar)\n",
    "print(f'The Prince to Interstellar similarity is {docsim12:.3}.')\n",
    "\n",
    "docsim13=prince.similarity(romeojuliet)\n",
    "print(f'The Prince to Romeo and Juliet similarity is {docsim13:.3}.')\n",
    "\n",
    "docsim23=interstellar.similarity(romeojuliet)\n",
    "print(f'Interstellar to Romeo and Juliet similarity is {docsim23:.3}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
