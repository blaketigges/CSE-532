{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Description: The 4 attached json files (sub_class0.json, sub_class1.json, sub_class2.json, sub_class3.json),\n",
    "represent four separate classes of top 100 submissions from subreddits with the following designations:\n",
    "sub_class0.json is taken from r/basketball; sub_class1.json is taken from r/machinelearning; sub_class2.json is\n",
    "taken from r/personalfinance; and sub_class3.json is taken from r/Louisville. Each individual submission has up\n",
    "to 5 characteristics stored as key-value pairs, with the keys being Title, id, Score, Num_Comments, and Date*.\n",
    "* Many submissions are missing one or more these characteristics: see instructions below.\n",
    "Instructions (Five parts in total):\n",
    "\n",
    "\n",
    "**Part 1.** For each json file, you are to take the following steps:\n",
    "* a. Load the json file into a Python data-frame (read_json is recommended, though you are welcome to use\n",
    "whatever approach preferred), and perform the following:\n",
    "* b. discard any rows that lack ANY of the 5 characteristics (Title, id, Score, Num_Comments, Date). Note that\n",
    "such rows will be missing the characteristics entirely (see the third submission within sub_class1.json as an\n",
    "example). Rows that include empty strings, 0 values, etc. should not be discarded for possessing them.\n",
    "* c. after doing so, discard the id characteristic, as it will not be used with future tasks.\n",
    "* d. Save this modified data-frame to a json file with the name sub_class#_P1.json, where # is the original json file\n",
    "id (ex: after modification, the data from sub_class0.json will be written to file sub_class0_P1.json, etc.) It is\n",
    "highly recommended that you use the data-frame to_json method for this task. If you do not, you must mention\n",
    "in your README or jupyter cells what alternative was used (so that we can properly read in your json file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load json files\n",
    "class0 = pd.read_json('sub_class0.json')\n",
    "class0.Name = \"sub_class0\"\n",
    "class1 = pd.read_json('sub_class1.json')\n",
    "class1.Name = \"sub_class1\"\n",
    "class2 = pd.read_json('sub_class2.json')\n",
    "class2.Name = \"sub_class2\"\n",
    "class3 = pd.read_json('sub_class3.json')\n",
    "class3.Name = \"sub_class3\"\n",
    "\n",
    "classes = [class0, class1, class2, class3]\n",
    "\n",
    "# Discard rows with missing columns\n",
    "for sub_class in classes:\n",
    "    sub_class.dropna(inplace=True)\n",
    "\n",
    "# Delete id column\n",
    "for sub_class in classes:\n",
    "    del sub_class['id']\n",
    "\n",
    "# re-export to json\n",
    "for sub_class in classes:\n",
    "    print( sub_class)\n",
    "    sub_class.to_json(sub_class.Name + '_P1.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Part 2.** For each modified data-frame (i.e. after the transformation from part 1) you are to take the following\n",
    "steps:\n",
    "* a. use textblob to calculate the sentiment polarity of each submission’s title (ignore the subjectivity, and don’t\n",
    "use the NaiveBayesAnalyzer option). You may want to store these in a structure (ex: list) to make step b simple.\n",
    "* b. Add a new column to your modified data-frame called “Sentiment,” which includes the value computed in\n",
    "step a above for each entry/row.\n",
    "* c. You are to then eliminate every row from your modified data-frame with “neutral” sentiment, defined to be\n",
    "any sentiment that falls between -0.1 and 0.1, inclusive (you must eliminate rows with 0.1 or -0.1 sentiment).\n",
    "* d. Save this modified data-frame to a json file with the name sub_class#_P2.json, where # is the original json file\n",
    "id (ex: after modification, the data from sub_class0.json will be written to file sub_class0_P2.json, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Load json files\n",
    "class0 = pd.read_json('sub_class0_P1.json')\n",
    "class0.Name = \"sub_class0\"\n",
    "class1 = pd.read_json('sub_class1_P1.json')\n",
    "class1.Name = \"sub_class1\"\n",
    "class2 = pd.read_json('sub_class2_P1.json')\n",
    "class2.Name = \"sub_class2\"\n",
    "class3 = pd.read_json('sub_class3_P1.json')\n",
    "class3.Name = \"sub_class3\"\n",
    "\n",
    "classes = [class0, class1, class2, class3]\n",
    "\n",
    "# Add polarity\n",
    "for sub_class in classes:\n",
    "    sub_class.insert(2, 'Sentiment', 0.0) # part b\n",
    "    for index, row in sub_class.iterrows(): # add sentiment polarity\n",
    "        blob = TextBlob(row['Title'])\n",
    "        sub_class.at[index, 'Sentiment'] = blob.sentiment.polarity # part a\n",
    "        # print(row['Title'], blob.sentiment.polarity)\n",
    "    for index, row in sub_class.iterrows(): # remove rows with neutral sentiment\n",
    "        if row['Sentiment'] >= -0.1 and row['Sentiment'] <= 0.1:\n",
    "            sub_class.drop(index, inplace=True) # part c\n",
    "    \n",
    "# re-export to json\n",
    "for sub_class in classes: # part d\n",
    "    sub_class.to_json(sub_class.Name + '_P2.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Part 3.** Using the data from step 2 (make sure you compute sentiment and exclude “neutral” submissions!),\n",
    "create a simple bar plot that depicts the following quantities related to sentiment in order (8 bars in total):\n",
    "* i. # of class0 submissions with positive sentiment, \n",
    "* ii. # of class0 submissions with negative sentiment\n",
    "* iii. # of class1 submissions with positive sentiment, \n",
    "* iv. # of class1 submissions with negative sentiment\n",
    "* v. # of class2 submissions with positive sentiment, \n",
    "* vi. # of class2 submissions with negative sentiment\n",
    "* vii. # of class3 submissions with positive sentiment, \n",
    "* viii. # of class3 submissions with negative sentiment\n",
    "You are free to use whatever presentation style you want for the bar plots, but make sure you include a legend or\n",
    "x-axis labels that clearly indicate which quantity each bar represents – and you must use the bar order above (left\n",
    "to right or top to bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load json files\n",
    "class0 = pd.read_json('sub_class0_P2.json')\n",
    "class0.Name = \"sub_class0\"\n",
    "class1 = pd.read_json('sub_class1_P2.json')\n",
    "class1.Name = \"sub_class1\"\n",
    "class2 = pd.read_json('sub_class2_P2.json')\n",
    "class2.Name = \"sub_class2\"\n",
    "class3 = pd.read_json('sub_class3_P2.json')\n",
    "class3.Name = \"sub_class3\"\n",
    "\n",
    "classes = [class0, class1, class2, class3]\n",
    "\n",
    "# sentiment distribution bar plot\n",
    "# count positve and negative for each class\n",
    "positive = {\n",
    "    \"sub_class0\": 0,\n",
    "    \"sub_class1\": 0,\n",
    "    \"sub_class2\": 0,\n",
    "    \"sub_class3\": 0\n",
    "}\n",
    "negative = {\n",
    "    \"sub_class0\": 0,\n",
    "    \"sub_class1\": 0,\n",
    "    \"sub_class2\": 0,\n",
    "    \"sub_class3\": 0\n",
    "}\n",
    "\n",
    "for sub_class in classes:\n",
    "    for index, row in sub_class.iterrows():\n",
    "        if row['Sentiment'] > 0:\n",
    "            positive[sub_class.Name] += 1\n",
    "        elif row['Sentiment'] < 0:\n",
    "            negative[sub_class.Name] += 1\n",
    "    \n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame([positive, negative], index=['Positive', 'Negative'])\n",
    "df = df.transpose()\n",
    "df.plot(kind='bar', ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Part 4.** Pool together all modified submissions from Part 2 into a single data-frame, but include an additional\n",
    "column named Class that corresponds to the original submission’s subreddit class (i.e. 0, 1, 2, or 3). Ex: If there\n",
    "are 37 submissions from the class2 subreddit in the combined data-frame, the Class column for each of the 37\n",
    "submissions should have a value of 2. Save this consolidated data-frame to a json file with the name\n",
    "sub_combined.json.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json files\n",
    "class0 = pd.read_json('sub_class0_P2.json')\n",
    "class0.Name = \"0\"\n",
    "class1 = pd.read_json('sub_class1_P2.json')\n",
    "class1.Name = \"1\"\n",
    "class2 = pd.read_json('sub_class2_P2.json')\n",
    "class2.Name = \"2\"\n",
    "class3 = pd.read_json('sub_class3_P2.json')\n",
    "class3.Name = \"3\"\n",
    "\n",
    "classes = [class0, class1, class2, class3]\n",
    "\n",
    "# combine and add class column\n",
    "combined = pd.DataFrame()\n",
    "for sub_class in classes:\n",
    "    sub_class.insert(0, 'Class', sub_class.Name)\n",
    "    combined = pd.concat([combined, sub_class])\n",
    "\n",
    "# re-export to json\n",
    "combined.to_json('sub_combined.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Part 5.** \n",
    "* i. Using your combined collection data-frame from Part 4, assuming it has n submissions in total, your next\n",
    "goal is to construct a n x 5 numpy feature array suited for machine learning. Each row in the array corresponds to\n",
    "a row in your data-frame, and the 5 array columns represent the features for the submission at that position as\n",
    "follows:\n",
    "* Feature 1: The character length of the submission’s title (don’t preprocess the title in any way!)\n",
    "* Feature 2: The sentiment of the submission’s title.\n",
    "* Feature 3: The submission’s Score.\n",
    "* Feature 4: The submission’s number of comments (Num_Comments).\n",
    "* Feature 5: The year of the submission (note there are several ways to extract the year from a Date in Python).\n",
    "For example, one row in your feature array may look like the below:\n",
    "[46. , -0.2 , 1356.0 , 249.0, 2020. ]\n",
    "meaning that the corresponding submission had a title with 46 characters, sentiment of -0.2, score of 1356, 249\n",
    "comments, and was posted in the year 2020.\n",
    "* ii. Now create a n x 1 numpy array using the Class column you added in Part 4. Each row here represents a target\n",
    "variable for classification that matches the corresponding submission. For example, if the first 46 rows in your\n",
    "data-frame from Part 4 correspond to class 0 submissions, then the first 46 entries in this array should likewise be 0.\n",
    "* iii. You are to then perform 10-fold cross-validation using one classification estimator (either one used during the\n",
    "course module on machine learning, or one of your own choosing) to determine the accuracy available in using\n",
    "the features from the nx5 array in predicting the Class given in the nx1 array. For full credit, you should produce\n",
    "both classifier accuracy for each cross-validation and a confusion matrix for the classifier predictions. You may\n",
    "want to look into using cross_val_predict for the latter. Write a few sentences (comments or a separate text file\n",
    "are fine for this) discussing whether or not you think the features and chosen classifiers provide acceptable\n",
    "accuracy for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "combined = pd.read_json('sub_combined.json')\n",
    "# print(combined)\n",
    "\n",
    "# make np array\n",
    "npCombine = np.zeros((len(combined), 5))\n",
    "for index, row in combined.iterrows():\n",
    "    npCombine[index][0] = len(row['Title'])\n",
    "    # print(len(row['Title']))\n",
    "    npCombine[index][1] = row['Sentiment']\n",
    "    # print(row['Sentiment'])\n",
    "    npCombine[index][2] = row['Score']\n",
    "    # print(row['Score'])\n",
    "    npCombine[index][3] = row['Num_Comments']\n",
    "    # print(row['Num_Comments'])\n",
    "    npCombine[index][4] = pd.to_datetime(row['Date'], unit='s').year\n",
    "    # print(pd.to_datetime(row['Date'], unit='s').year)\n",
    "\n",
    "# np.set_printoptions(suppress=True)\n",
    "# print(npCombine)\n",
    "\n",
    "# np array of class\n",
    "npClass = np.array(combined['Class'])\n",
    "\n",
    "# print(npClass)\n",
    "\n",
    "# 10 fold cross validation to determine accuracy that first 5 features can predict class\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=10, random_state=11, shuffle=True)\n",
    "scores = cross_val_score(knn, npCombine, npClass, cv=kfold)\n",
    "print(f'K Neighbors Classifier: ' +\n",
    "          f'mean accuracy={scores.mean():.2%}, ' +\n",
    "          f'standard deviation={scores.std():.2%}')\n",
    "\n",
    "predicted = cross_val_predict(knn, npCombine, npClass, cv=kfold)\n",
    "conf_matrix = confusion_matrix(npClass, predicted)\n",
    "print(\"\\nConfusion Matrix: \")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if I did this right but the results make it seem like these features are good at predicted the class of the subreddit. I would have thought the text of the title would be the only data useful for predicted where its from but it seems like the model is able to figure out something I can't see. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs532",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
