{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Load the words of this structure in sequential order of appearance into a one-dimensional Python list (i.e. the first word should be the first element in the list, while the last word should be the last element) that is **case insensitive**.  It's up to you how to deal with special chacters -- you can remove them manually, ignore them during the loading process, or even count them as words, for example.  **Make sure you have this list clearly assigned to a variable, so we can evaluate it during grading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "file = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "words = []\n",
    "for line in file:\n",
    "    # print(line)\n",
    "    for word in line.split():\n",
    "        # print(word.lower())\n",
    "        words.append(word.lower())\n",
    "# print(words)\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].strip(string.punctuation)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use your list to create **and print** a two-column pandas data-frame with the following properties: <br>\n",
    "i. The first column for each index should represent the word in question at that index <br>\n",
    "ii. The second column should represent the number of times that particular word appears in the text. <br>\n",
    "iii. The rows of the data-frame should be ordered according to the **first** occurrence of each word. <br>\n",
    "iv. It's up to you whether or not your data-frame will include an index per row.  <br>**Make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.**\n",
    "\n",
    "Ex: if the first word in your text is \"the\" which occurs 500 times and the second is \"balcony\" which only appears twice, your data-frame should _begin_ like the following:\n",
    "\n",
    "| | Word | Count |\n",
    "| --- | --- | --- |\n",
    "| 1 | \"the\" | 500 |\n",
    "| 2 | \"balcony\" | 2 |\n",
    "| ... | ... | ... |\n",
    "Again, the indices are optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "file = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "words = []\n",
    "for line in file:\n",
    "    for word in line.split():\n",
    "        # print(word.lower())\n",
    "        words.append(word.lower())\n",
    "# print(words)\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].strip(string.punctuation)\n",
    "dataframe = pd.DataFrame(words, columns=['word'])\n",
    "print(dataframe['word'].value_counts(sort=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) **Stop-words** are commonly used words in a given language that often fail to communicate useful summative information about its content.  The attached file \"stop_words.txt\" has a simple list of common stop words assigned to a variable.  For this part of the assigment, you are to create a modified copy of the data-frame from (c) with the following modifications: _i. all stop words have been removed from the data-frame_ and _ii. the data frame rows have been sorted in decreasing order of frequency counts_.  **Again, make sure you have this data-frame clearly assigned to a variable, so we can evaluate it during grading.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "file = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "words = []\n",
    "x = 0\n",
    "for line in file:\n",
    "    if \"BY THE DUKE VALENTINO WHEN MURDERING\" in line: # filter out the extra stuff at the end\n",
    "        print(line)\n",
    "        if x == 0:\n",
    "            x = 1\n",
    "        else:\n",
    "            break\n",
    "    for word in line.split():\n",
    "        # print(word.lower())\n",
    "        words.append(word.lower()) # convert to lowercase and append to list\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].strip(string.punctuation) # remove punctuation\n",
    "\n",
    "stopwords = open('stop_words.txt', 'r')\n",
    "stop_words = [] # create list before exec to stop the red lines\n",
    "exec(stopwords.read())\n",
    "    \n",
    "dataframe = pd.DataFrame(words, columns=['word'])\n",
    "dataframe = dataframe[~dataframe['word'].isin(stop_words)] # remove stop words\n",
    "print(dataframe['word'].value_counts(ascending=False).head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) While total word counts can provide a useful measure of the content of a document, they cannot reveal much about its underlying trends.  In the context of document analysis, the term _trend_ implies a direction (in terms of theme, mood, etc.) in which the content changes throughout the narrative.  For example, some works of fiction begin with a comedic tone, and take on a more serious tone in later stages, or vice versa.  While tone shifts are a bit beyond the scope of this assignment, it *is* feasible to monitor which **words** become more or less common within the document over time (ex: a character killed in the first act of a screenplay will *potentially* appear less frequently in subsequent acts).  For the last part of your assignment, you are to do the following: <br>\n",
    "i. Use your dataframe from part d to pinpoint the six most commonly occurring words in your text (if there are stop-words that were not successfully excluded programmatically in part d due to punctuation, etc., you should manually exclude them here). <br>\n",
    "ii. For each of the six words, compute the ratio of its appearances per narrative segment (chapter, act, etc.) compared to the sum across the work overall.  These quantities can be either in percentile or decimal. <br>\n",
    "iii. You are then to create a 2 x 3 or 3 x 2 series of subplots and, for each of the six words, provide a bar graph that depicts the ratios (not raw counts) across segments.\n",
    "\n",
    "As an example, suppose a hypothetical work has five chapters, and one of the six most common words is \"Barcelona\"; further, suppose this word appears 10, 20, 15, 5, and 0 times in chapters 1, 2, 3, 4, and 5 respectively.  The bar graph for this plot should look something like the below: <br>\n",
    "![Barcelona Trends Bar Graph](barcelonatrends.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locale import normalize\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "file = open('pg1232.txt', 'r', encoding='utf-8-sig')\n",
    "words = []\n",
    "chapter = \"0\"\n",
    "temp = []\n",
    "x = 0\n",
    "for line in file:\n",
    "    if \"BY THE DUKE VALENTINO WHEN MURDERING\" in line: # filter out the extra stuff at the end\n",
    "        if x == 0:\n",
    "            x = 1\n",
    "        else:\n",
    "            break\n",
    "    if 'CHAPTER' in line:\n",
    "        pos = line.split(sep=\".\")[0].split()[1]\n",
    "        if pos in temp: # Dont change chapter during table of contents\n",
    "            chapter = pos\n",
    "        else:\n",
    "            temp.append(pos)\n",
    "    for word in line.split():\n",
    "        if chapter != \"0\":\n",
    "            words.append([word.lower(), chapter]) # convert to lowercase and append to list if chapter is not 0\n",
    "\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i][0] = words[i][0].strip(string.punctuation) # remove punctuation\n",
    "\n",
    "stopwords = open('stop_words.txt', 'r')\n",
    "stop_words = [] # create list before exec to stop the red lines\n",
    "exec(stopwords.read()) \n",
    "    \n",
    "dataframe = pd.DataFrame(words, columns=['Word', 'Chapter'])\n",
    "dataframe = dataframe[~dataframe['Word'].isin(stop_words)] # remove stop words\n",
    "# print(dataframe['Word'].value_counts(ascending=False).head(6))\n",
    "\n",
    "# Calculate word frequency by chapter\n",
    "for word in dataframe['Word'].value_counts(ascending=False).head(6).index: # iterate through top 6 words\n",
    "    # print(word)\n",
    "    fig, ax = plt.subplots() \n",
    "    chapter_counts = dataframe[dataframe['Word'] == word]['Chapter'].value_counts(normalize=True).mul(100).sort_index() # calculate frequency by chapter\n",
    "    chapter_counts.plot(kind='bar', ax=ax, title=word.capitalize() + ' Frequency by Chapter') # plot frequency by chapter\n",
    "    ax.set_ylabel('Frequency (%)') \n",
    "    ax.set_xlabel('Chapter #')\n",
    "    plt.show() # show plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
